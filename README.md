# Robustness of feed-forward neural networks against adversarial examples

This repository contains the codes developed and used in my master's thesis in engineering. The codes were employed to generate all the results. If you want to review the thesis associated with this repository, you can find it at this [link](https://ricabib.cab.cnea.gov.ar/1197/1/1Kloster.pdf). Please note that the only available version is in Spanish. However, it's important to mention that the code has not been extensively refined, so the setup to run certain parts may fail due to dependencies. I apologize for any inconvenience this may cause.

In this repository, you'll find the essential files listed below. Not all files and folders are included here, as some are datasets that need to be downloaded or generated using `robustness.py`, which will be explained later.

``` text
.
├── Codes/
│   ├── attacks/                    (Attack codes to generate adversarial images)
│   │   ├── deepfool.py
│   │   ├── FGSM.py
│   │   └── CW2.py
│   ├── gen_data/                   (Data generated by robustness.py)
│   │   └── ...
│   ├── lib/                        (Libraries)
│   │   ├── ConvolutionalNoise.py
│   │   ├── DenseNoise.py
│   │   └── functions.py
│   ├── models/                     (Neural networks models)
│   │   ├── MNIST-model_1.py
│   │   ├── CIFAR-model_1.py
│   │   ├── DR-model_1.py
│   │   ├── smallDR-model_1.py
│   │   └── smallDR-model_2.py
│   ├── execute                     (Useful to run codes in cluster)
│   └── robustness.py               (Main file to run, explanation below)
├── Datasets/
│   └── DR/                         (Dataset of diabetic retinopathy, must be downloaded from Kaggle)
│       ├── train_small_100_150/
│       │   └── ...
│       └── train_small_200_300/
│           └── ...
├── Notebooks/
│   ├── SNR.ipynb                   (Used to calculate SNR)
│   ├── central_monitor.ipynb       (Used to monitor all the results generated by robustness.py)
│   ├── colorgraph_limits.ipynb
│   └── graphs/                     (Graphs generated by central_monitor.ipynb)
│       └── ...
└── README.md
```

## How to run robustness.py

The file _robustness.py_ can be used for three different purposes and needs `tensorflow > 2.0`. Click on the one you want to know more about:

<details><summary>Train only (click me)</summary>

The general command for **training** is:

``` bash
python robustness.py -ds <DATASET> -m <MODEL>
```

| Name      | Possible values                            |
| :-------- | :----------------------------------------- |
| `DATASET` | `MNIST` or `CIFAR10` or `DR` or `smallDR`  |
| `MODEL`   | Depends on models saved on models folder   |

As an example, lets run the dataset `DR` training it with model `1`, i.e. the network that is in _Codes/models/DR-model_1.py_

``` bash
python robustness.py -ds DR -m 1
```

</details>

<details><summary>Train and attack only (click me)</summary>

The general command for **training and attacking** is:

``` bash
python robustness.py -ds <DATASET> -m <MODEL> -a <ATTACK> -<ATTACK_PRMTR_NAME> <ATTACK_PRMTR_VALUE> -mi <MAX_NB_OF_ATTACK_ITERS> -ns <NUMBER_OF_SUCC_ADV_IMAGES>
```

| Name                        | Possible values                                    |
| :-------------------------- | :------------------------------------------------- |
| `ATTACK`                    | `deepfool` or `FGSM` or `CW2`                      |
| `ATTACK_PRMTR_NAME`         | `eta` or `eps` or (`kappa` and `cte` and `cteupd`) |
| `MAX_NB_OF_ATTACK_ITERS`    | integer value (20 by default)                      |
| `NUMBER_OF_SUCC_ADV_IMAGES` | integer value (50 by default)                      |

As an example, lets run the dataset `CIFAR10` training it with model `1`, i.e. the network that is in _Codes/models/CIFAR10-model_1.py_. Then, attack the correct classified images with the `CW2` attack, with some `kap` `cte` and `cteupd` parameters. Also the optional parameters `-ns <MAX_NB_OF_ATTACK_ITERS>` and `-mi <NUMBER_OF_SUCC_ADV_IMAGES>` are specified.

``` bash
python robustness.py -ds CIFAR10 -m 1 -a CW2 -kap 0.1 -cte 0.0032 -cteupd 0.0032 -ns 2000 -mi 20
```

</details>

<details><summary>Train, attack and detection (click me)</summary>


The general command for **training, attacking and detection** is:

``` bash
python robustness.py -ds <DATASET> -m <MODEL> -a <ATTACK> -<ATTACK_PRMTR_NAME> <ATTACK_PRMTR_VALUE> -mi <MAX_NB_OF_ATTACK_ITERS> -ns <NUMBER_OF_SUCC_ADV_IMAGES> -d <DETECTION_METHOD> -xs <X_START_VALUE> -xe <X_END_VALUE> -dx <X_STEP> -ys <Y_START_VALUE> -ye <Y_END_VALUE> -dy <Y_STEP> -ni <NOISE_ITER>
```

| Name               | Possible values                |
| :----------------- | :----------------------------- |
| `DETECTION_METHOD` | `stoGauNet`                    |
| `X_START_VALUE`    | float value                    |
| `X_END_VALUE`      | float value                    |
| `X_STEP`           | float value                    |
| `Y_START_VALUE`    | float value                    |
| `Y_END_VALUE`      | float value                    |
| `Y_STEP`           | float value                    |
| `NOISE_ITER`       | integer value (100 by default) |

As an example, let's run the dataset `MNIST` training it with model `1`, i.e. the network that is in _Codes/models/MNIST-model_1.py_. Then, attack the correct classified images with the `deepfool` attack, with an `eta` parameter of 0.1. Finally, use the detection method Stochastic Gaussian Network (`stoGauNet`) with the some specified grid.

``` bash
python robustness.py -ds MNIST -m 1 -a deepfool -eta 0.1 -d stoGauNet -xs 0.0 -xe 1.8 -dx 0.2 -ys 0.0 -ye 0.18 -dy 0.02
```

</details>

## How to monitor the results generated by robustness.py

Just run _Notebooks/central_monitor.ipynb_ and the data will be saved in Notebooks/graphs/

## Recommended next steps

Due to time constraints during the thesis work, certain recommended steps were not completed. These steps involve refining the codebase for better readability and scalability.

<details><summary>Click me for more details</summary>
  
In `Codes/robustness.py` where datasets are loaded, it would be beneficial to encapsulate that in another module. By doing so, one could simply call the object and/or functions to obtain the train and test databases. 

Similarly, in the section where the detection is executed, specifically in the code block `if args.detection == "stoGauNet"`, it would be advantageous to encapsulate this elsewhere. Even though it is currently the only detection method in this repository, structuring it more flexibly would be beneficial in case additional methods are added.

Lastly, the attacks located in `Codes/attacks/<name_of_attack>.py` are implemented in `Codes/robustness.py` using the approach `exec(open(path_code_attack).read())`, which is not considered best practice. Therefore, it would be wise to package the attacks and call them as functions or methods of an object.
</details>
