{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def compute_SNR(attack_path, n_layer=-2, adv = False):\n",
    "  # Load the data from npz file\n",
    "  data = np.load(attack_path)\n",
    "  X_nat = data[\"X_test_nat_adv\"][0]\n",
    "  X_adv = data[\"X_test_nat_adv\"][1]\n",
    "\n",
    "  if adv:\n",
    "    X_nat, X_adv = X_adv, X_nat\n",
    "\n",
    "  # Load the model\n",
    "  model_path = \"-\".join(attack_path.split(\"-\")[:-1]) + \"-model.h5\"\n",
    "  model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "  # `n_layer` BE CAREFUL with this, it is the layer number, not the index. Check the model summary to get the layer number\n",
    "\n",
    "  # print(\"Model summary: \")\n",
    "  # model.summary()\n",
    "\n",
    "  # Get output of layer n_layer when the input to model is X_nat\n",
    "  layer_output = model.layers[n_layer].output\n",
    "  intermediate = tf.keras.models.Model(inputs=model.input, outputs=layer_output)\n",
    "  intermediate_output_nat = intermediate.predict(X_nat)\n",
    "  intermediate_output_adv = intermediate.predict(X_adv)\n",
    "\n",
    "  # if the dimension of the intermediate output is greater than 1, then we need to flatten it\n",
    "  if len(intermediate_output_nat.shape) > 2:\n",
    "    new_shape = intermediate_output_nat.shape[0],np.prod(intermediate_output_nat.shape[1:])\n",
    "    intermediate_output_nat = intermediate_output_nat.reshape(new_shape)\n",
    "    new_shape = intermediate_output_adv.shape[0],np.prod(intermediate_output_adv.shape[1:])\n",
    "    intermediate_output_adv = intermediate_output_adv.reshape(new_shape)\n",
    "  \n",
    "\n",
    "  N_features = intermediate_output_nat.shape[1] # Number of features in the intermediate output\n",
    "\n",
    "  # Remove centroid from intermediate outputs\n",
    "  centroid_nat = np.mean(intermediate_output_nat, axis=0)\n",
    "  centroid_adv = np.mean(intermediate_output_adv, axis=0)\n",
    "  intermediate_output_nat = intermediate_output_nat - centroid_nat\n",
    "  intermediate_output_adv = intermediate_output_adv - centroid_adv\n",
    "\n",
    "  # intermediate_outputs are matrices of shape (n_samples, n_features). Get the covariance matrix\n",
    "  cov_nat = np.cov(intermediate_output_nat, rowvar=False)\n",
    "  cov_adv = np.cov(intermediate_output_adv, rowvar=False)\n",
    "\n",
    "  # Get the eigenvalues and eigenvectors of the covariance matrices\n",
    "  eigval_nat, eigvec_nat = np.linalg.eig(cov_nat)\n",
    "  eigval_adv, eigvec_adv = np.linalg.eig(cov_adv)\n",
    "\n",
    "  # Calculate the radius of intermediate_output_nat along the eigenvectors\n",
    "  Rs_nat = np.zeros((eigvec_nat.shape[0],))\n",
    "  for i, vec in enumerate(eigvec_nat.T):\n",
    "    dot_prod = np.dot(intermediate_output_nat, np.conj(vec))\n",
    "    Rs_nat[i] = np.linalg.norm(dot_prod)\n",
    "  R_nat2 = 1 / N_features * np.sum(np.square(Rs_nat))\n",
    "\n",
    "  # Calculate the radius of intermediate_output_adv along the eigenvectors\n",
    "  Rs_adv = np.zeros((eigvec_adv.shape[0],))\n",
    "  for i, vec in enumerate(eigvec_adv.T):\n",
    "    dot_prod = np.dot(intermediate_output_adv, np.conj(vec))\n",
    "    Rs_adv[i] = np.linalg.norm(dot_prod)\n",
    "  R_adv2 = 1 / N_features * np.sum(np.square(Rs_adv))\n",
    "\n",
    "  # terms to calculate SNR\n",
    "  term_signal = np.linalg.norm(centroid_nat - centroid_adv)**2\n",
    "  term_signal /= R_nat2\n",
    "  print(\"term_signal: \", term_signal)\n",
    "\n",
    "  term_bias = ( R_adv2/R_nat2 - 1 )\n",
    "  m_examples = intermediate_output_nat.shape[0]\n",
    "  term_bias /= m_examples\n",
    "  print(\"term_bias: \", term_bias)\n",
    "\n",
    "  term_dimension = ((R_nat2**2) / np.sum(Rs_nat**4)) ** (-1)\n",
    "  term_dimension /= m_examples\n",
    "  print(\"term_dimension: \", term_dimension)\n",
    "\n",
    "  delta_centroids = centroid_nat - centroid_adv\n",
    "\n",
    "  \n",
    "  U_adv = (eigvec_adv * Rs_adv)/(R_adv2**0.5)\n",
    "  term_SN_overlap_adv = np.linalg.norm(np.matmul(delta_centroids,U_adv))**2\n",
    "  term_SN_overlap_adv /= m_examples \n",
    "  print(\"term_SN_overlap_adv: \", term_SN_overlap_adv)\n",
    "\n",
    "  U_nat = (eigvec_nat * Rs_nat)/(R_nat2**0.5)\n",
    "  term_SN_overlap_nat = np.linalg.norm(np.matmul(delta_centroids,U_nat))**2\n",
    "  print(\"term_SN_overlap_nat: \", term_SN_overlap_nat)\n",
    "\n",
    "  SNR_num = (term_signal + term_bias)\n",
    "  SNR_den = (term_dimension + term_SN_overlap_adv + term_SN_overlap_nat) ** (0.5)\n",
    "\n",
    "  return 0.5 * SNR_num / SNR_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"dataset\",\"attack\"])\n",
    "\n",
    "data = [    \n",
    "   {\"dataset\": \"MNIST\", \"attack\": \"deepfool\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -7, \"path\": \"../Codes/gen_data/MNIST-model_1-deepfool_eta_0_01_mi_20_ns_50.npz\"},\n",
    "   {\"dataset\": \"MNIST\", \"attack\": \"FGSM\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -7, \"path\": \"../Codes/gen_data/MNIST-model_1-FGSM_eps_0_062_ns_50.npz\"},\n",
    "   {\"dataset\": \"MNIST\", \"attack\": \"CW2\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -7, \"path\": \"../Codes/gen_data/MNIST-model_1-CW2_kap_0_1_cte_0_011_cteupd_0_011_mi_20_ns_50.npz\"},\n",
    "\n",
    "   {\"dataset\": \"CIFAR10\", \"attack\": \"deepfool\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -7, \"path\": \"../Codes/gen_data/CIFAR10-model_1-deepfool_eta_0_45_mi_20_ns_50.npz\"},\n",
    "   {\"dataset\": \"CIFAR10\", \"attack\": \"FGSM\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -7, \"path\": \"../Codes/gen_data/CIFAR10-model_1-FGSM_eps_0_005_ns_50.npz\"},\n",
    "   {\"dataset\": \"CIFAR10\", \"attack\": \"CW2\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -7, \"path\": \"../Codes/gen_data/CIFAR10-model_1-CW2_kap_0_1_cte_0_0032_cteupd_0_0032_mi_20_ns_50.npz\"},\n",
    "\n",
    "   {\"dataset\": \"DR\", \"attack\": \"deepfool\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -11, \"path\": \"../Codes/gen_data/DR-model_1-deepfool_eta_5_0_mi_20_ns_50.npz\"},\n",
    "   {\"dataset\": \"DR\", \"attack\": \"FGSM\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -11, \"path\": \"../Codes/gen_data/DR-model_1-FGSM_eps_0_001_ns_50.npz\"},\n",
    "   {\"dataset\": \"DR\", \"attack\": \"CW2\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -11, \"path\": \"../Codes/gen_data/DR-model_1-CW2_kap_0_1_cte_0_0005_cteupd_0_0005_mi_20_ns_50.npz\"},\n",
    "\n",
    "   {\"dataset\": \"smallDR\", \"attack\": \"deepfool\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -11, \"path\": \"../Codes/gen_data/smallDR-model_2-deepfool_eta_30_0_mi_20_ns_50.npz\"},\n",
    "   {\"dataset\": \"smallDR\", \"attack\": \"FGSM\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -11, \"path\": \"../Codes/gen_data/smallDR-model_2-FGSM_eps_0_001_ns_50.npz\"},\n",
    "   {\"dataset\": \"smallDR\", \"attack\": \"CW2\", \"idx_noisy_layer_dense\": -4, \"idx_noisy_layer_conv\": -11, \"path\": \"../Codes/gen_data/smallDR-model_2-CW2_kap_0_1_cte_0_017_cteupd_0_017_mi_50_ns_50.npz\"},\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for d in data:\n",
    "    dfs.append(pd.DataFrame(d, index=[0]))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# add columns for SNR with nan values\n",
    "df_columns_to_add = [\"Dense SNR_nat\", \"Conv SNR_nat\", \"Dense SNR_adv\", \"Conv SNR_adv\"]\n",
    "for col in df_columns_to_add:\n",
    "    df[col] = np.nan\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill table with SNR values\n",
    "for idx, row in df.iterrows():\n",
    "  # df.loc[idx, \"Dense SNR_nat\"] = compute_SNR(row[\"path\"], row[\"idx_noisy_layer_dense\"])\n",
    "  # df.loc[idx, \"Dense SNR_adv\"] = compute_SNR(row[\"path\"], row[\"idx_noisy_layer_dense\"], adv=True)\n",
    "  if idx != 9:\n",
    "    continue\n",
    "  df.loc[idx, \"Conv SNR_nat\"] = compute_SNR(row[\"path\"], row[\"idx_noisy_layer_conv\"])\n",
    "  # df.loc[idx, \"Conv SNR_adv\"] = compute_SNR(row[\"path\"], row[\"idx_noisy_layer_conv\"], adv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now manually, add a column \"p_det\" to df, which is the probability of detection for each attack on each dataset. So first add that column with zeros\n",
    "df.loc[(df[\"dataset\"] == \"MNIST\") & (df[\"attack\"] == \"deepfool\"), \"p_det_val\"] = 0.99\n",
    "df.loc[(df[\"dataset\"] == \"MNIST\") & (df[\"attack\"] == \"FGSM\"), \"p_det_val\"] = 0.80\n",
    "df.loc[(df[\"dataset\"] == \"MNIST\") & (df[\"attack\"] == \"CW2\"), \"p_det_val\"] = 0.98\n",
    "\n",
    "df.loc[(df[\"dataset\"] == \"CIFAR10\") & (df[\"attack\"] == \"deepfool\"), \"p_det_val\"] = 0.89\n",
    "df.loc[(df[\"dataset\"] == \"CIFAR10\") & (df[\"attack\"] == \"FGSM\"), \"p_det_val\"] = 0.59\n",
    "df.loc[(df[\"dataset\"] == \"CIFAR10\") & (df[\"attack\"] == \"CW2\"), \"p_det_val\"] = 0.87\n",
    "\n",
    "df.loc[(df[\"dataset\"] == \"DR\") & (df[\"attack\"] == \"deepfool\"), \"p_det_val\"] = 0.54\n",
    "df.loc[(df[\"dataset\"] == \"DR\") & (df[\"attack\"] == \"FGSM\"), \"p_det_val\"] = 0.43\n",
    "df.loc[(df[\"dataset\"] == \"DR\") & (df[\"attack\"] == \"CW2\"), \"p_det_val\"] = 0.74\n",
    "\n",
    "df.loc[(df[\"dataset\"] == \"smallDR\") & (df[\"attack\"] == \"deepfool\"), \"p_det_val\"] = 0.55\n",
    "df.loc[(df[\"dataset\"] == \"smallDR\") & (df[\"attack\"] == \"FGSM\"), \"p_det_val\"] = 0.49\n",
    "df.loc[(df[\"dataset\"] == \"smallDR\") & (df[\"attack\"] == \"CW2\"), \"p_det_val\"] = 0.55\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to csv\n",
    "df.to_csv(\"table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "axis_cols = 2\n",
    "axis_rows = int(np.ceil(len(i_layers_to_test)/axis_cols))\n",
    "\n",
    "fig, ax = plt.subplots(axis_rows, axis_cols, figsize=(15, 15),squeeze=False)\n",
    "for i,i_layer in enumerate(i_layers_to_test[::-1]):\n",
    "  ax_i = ax[i//axis_cols, i%axis_cols]\n",
    "  ax_i.set_xlabel(\"SNR\", fontsize=16)\n",
    "  # ylabel using the latex syntax\n",
    "  ax_i.set_ylabel(r\"$p_{det}$\", fontsize=16)\n",
    "  ax_i.scatter(df[\"SNR layer \" + str(i_layer)], df[\"p_det_val\"])\n",
    "\n",
    "  # Add a line for the best linear fit\n",
    "  snrs = df[\"SNR layer \" + str(i_layer)].to_numpy()\n",
    "  p_dets = df[\"p_det_val\"].to_numpy()\n",
    "\n",
    "  # cast to float\n",
    "  snrs = snrs.astype(float)\n",
    "  p_dets = p_dets.astype(float)\n",
    "\n",
    "  a, b = np.polyfit(snrs, p_dets, 1)\n",
    "  r = np.corrcoef(snrs, p_dets)[0, 1]\n",
    "\n",
    "  # Plot the line\n",
    "  ax_i.plot(snrs, a*snrs + b, color='gray', label='y = {:.2f}x + {:.2f}'.format(a, b))\n",
    "  ax_i.legend()\n",
    "\n",
    "  # calculate best fit line\n",
    "  slope, intercept, r_value, p_value, std_err = linregress(snrs, p_dets)\n",
    "\n",
    "  ax_i.set_title(r\"SNR layer index: \" + str(i_layer) + \" | $(R = {:.2f})$\".format(r), fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "path_history_save = \"./graphs/SNRs.pdf\"\n",
    "fig.savefig(path_history_save, bbox_inches=\"tight\", pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0fa19f39feed0e28ef7441c3b4072640a83d1e2ae2b07ccfb105f15eed61b39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
