# Training parameters
batch_size = 128
nb_epoch = 50
nb_filters = 32
pool_size = (3, 3)
kernel_size = (3, 3)
learning_rate = 2.8935978459231752e-05
weight_decay =  6.59865797052157e-06
beta_1 = 0.7093948167818289

dropout_rate = 0.001524306199197037
# Model
training = False
if(not ('model' in locals())):
    training = True
    print("Creating model")

    model = tf.keras.models.Sequential()

    model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=input_shape))
    model.add(tf.keras.layers.Activation('relu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Dropout(dropout_rate))

    model.add(tf.keras.layers.Conv2D(64, (3, 3)))
    model.add(tf.keras.layers.Activation('relu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Dropout(dropout_rate))

    model.add(tf.keras.layers.Conv2D(128, (3, 3)))
    model.add(tf.keras.layers.Activation('relu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Dropout(dropout_rate))

    # suggest dense_n_neurons to be 512, 1024, 2048
    dense_n_neurons = 1024

    # add a convolutional layer with 1024 filters of size 3x3 sometimes, depending on the trial
    model.add(tf.keras.layers.Flatten())

    model.add(tf.keras.layers.Dense(dense_n_neurons))
    model.add(tf.keras.layers.Activation('relu'))

    model.add(tf.keras.layers.Dense(dense_n_neurons))
    model.add(tf.keras.layers.Activation('relu'))

    model.add(tf.keras.layers.Dense(nb_classes))
    model.add(tf.keras.layers.Activation('softmax'))

    # Let's train the model
    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=weight_decay, beta_1=beta_1)

    model.compile(loss='categorical_crossentropy',
            optimizer=opt,
            metrics=['accuracy'])

    steps_per_epoch = len(X_train)/batch_size

    datagen = ImageDataGenerator(rotation_range=30,
                                    width_shift_range=0.1,
                                    height_shift_range=0.1,
                                    shear_range=0.2,
                                    zoom_range=0.2,
                                    horizontal_flip=True,
                                    featurewise_center=False,
                                    featurewise_std_normalization = False,
                                    fill_mode="nearest")

    # Compute quantities required for featurewise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
    datagen.fit(X_train)
    #X_test = (X_test - datagen.mean)/(datagen.std + 1e-7)

    # Fit the model on the batches generated by datagen.flow().
    history=model.fit_generator(
                        datagen.flow(X_train,Y_train, batch_size=batch_size),
                        epochs=nb_epoch,
                        validation_data=(X_test,Y_test),
                        verbose=verbose,
                        steps_per_epoch=steps_per_epoch)

# Model 1
if((not training) and args.detection == "stoGauNet"):
    if('model1' in locals()):
        # modify the model1
        model1.get_layer("conv_noisy").sigmanoise = sigmanoiseC
        model1.get_layer("dense_noisy").sigmanoise = sigmanoiseD
    else:
        model1 = tf.keras.models.Sequential()

        model1.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=input_shape))
        model1.add(tf.keras.layers.Activation('relu'))
        model1.add(tf.keras.layers.BatchNormalization())
        model1.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))
        model1.add(tf.keras.layers.Dropout(dropout_rate))

        model1.add(tf.keras.layers.Conv2D(64, (3, 3)))
        model1.add(tf.keras.layers.Activation('relu'))
        model1.add(tf.keras.layers.BatchNormalization())
        model1.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))
        model1.add(tf.keras.layers.Dropout(dropout_rate))

        model1.add(Conv2DNoise(128, (3, 3),sigmanoise=sigmanoiseC, name="conv_noisy"))
        model1.add(tf.keras.layers.Activation('relu'))
        model1.add(tf.keras.layers.BatchNormalization())
        model1.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))
        model1.add(tf.keras.layers.Dropout(dropout_rate))

        # suggest dense_n_neurons to be 512, 1024, 2048
        dense_n_neurons = 1024

        # add a convolutional layer with 1024 filters of size 3x3 sometimes, depending on the trial
        model1.add(tf.keras.layers.Flatten())

        model1.add(tf.keras.layers.Dense(dense_n_neurons))
        model1.add(tf.keras.layers.Activation('relu'))

        model1.add(DenseNoise(dense_n_neurons,sigmanoise=sigmanoiseD, name="dense_noisy"))
        model1.add(tf.keras.layers.Activation('relu'))

        model1.add(tf.keras.layers.Dense(nb_classes, name="last_layer"))
        model1.add(tf.keras.layers.Activation('softmax'))

        # Let's train the model
        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=weight_decay, beta_1=beta_1)
        model1.compile(loss='categorical_crossentropy',
                optimizer=opt,
                metrics=['accuracy'])